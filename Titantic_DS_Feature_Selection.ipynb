{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from random import seed, randint\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import randint\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import seaborn as sns\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from random import seed, randint\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.preprocessing import Imputer \n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "#train_df = train_df_copy.copy(deep = True)\n",
    "# Two optional arguments that you can add to .read_csv() \n",
    "    # na_values: Converts a given string to np.nan, defaults to None\n",
    "        #Add the argument na_values=['NAN'] to pd.read_csv().\n",
    "    # parse_dates: Reads the data in a list of given columns as dtype datetime64, defaults to False\n",
    "        #Add parse_dates=['Last Update'] to pd.read_csv()\n",
    "Id = test_df['PassengerId']\n",
    "data_cleaner = [train_df, test_df]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#remove Y variable\n",
    "y = train_df[\"Survived\"]\n",
    "train_df = train_df.drop([\"Survived\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train columns with null values:\\n', PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64)\n",
      "----------\n",
      "('Test/Validation columns with null values:\\n', PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print('Train columns with null values:\\n', train_df.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test/Validation columns with null values:\\n', test_df.isnull().sum())\n",
    "print(\"-\"*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###COMPLETING: complete or delete missing values in train and test/validation dataset\n",
    "drop_column = ['PassengerId','Cabin', 'Ticket']\n",
    "for dataset in data_cleaner:    \n",
    "    #complete missing age with median\n",
    "    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n",
    "\n",
    "    #complete embarked with mode\n",
    "    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n",
    "\n",
    "    #complete missing fare with median\n",
    "    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n",
    "    #delete the cabin feature/column and others previously stated to exclude in train dataset\n",
    "    dataset.drop(drop_column, axis=1, inplace = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    0\n",
      "Pclass      0\n",
      "Name        0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "----------\n",
      "Pclass      0\n",
      "Name        0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Title_Dictionary = {\n",
    "\"Capt\": \"Officer\",\n",
    "\"Col\": \"Officer\",\n",
    "\"Major\": \"Officer\",\n",
    "\"Jonkheer\": \"Royalty\",\n",
    "\"Don\": \"Royalty\",\n",
    "\"Sir\" : \"Royalty\",\n",
    "\"Dr\": \"Officer\",\n",
    "\"Rev\": \"Officer\",\n",
    "\"the Countess\": \"Royalty\",\n",
    "\"Dona\": \"Royalty\",\n",
    "\"Mme\": \"Mrs\",\n",
    "\"Mlle\": \"Miss\",\n",
    "\"Ms\": \"Miss\",\n",
    "\"Mr\" : \"Mr\",\n",
    "\"Mrs\" : \"Mrs\",\n",
    "\"Miss\" : \"Miss\",\n",
    "\"Master\" : \"Master\",\n",
    "\"Lady\" : \"Royalty\"\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\203014767\\.conda\\envs\\DAND\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 14 columns):\n",
      "Survived      891 non-null int64\n",
      "Pclass        891 non-null int64\n",
      "Name          891 non-null object\n",
      "Sex           891 non-null object\n",
      "Age           891 non-null float64\n",
      "SibSp         891 non-null int64\n",
      "Parch         891 non-null int64\n",
      "Fare          891 non-null float64\n",
      "Embarked      891 non-null object\n",
      "FamilySize    891 non-null int64\n",
      "IsAlone       891 non-null int64\n",
      "Title         891 non-null object\n",
      "FareBin       891 non-null category\n",
      "AgeBin        891 non-null category\n",
      "dtypes: category(2), float64(2), int64(6), object(4)\n",
      "memory usage: 85.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 13 columns):\n",
      "Pclass        418 non-null int64\n",
      "Name          418 non-null object\n",
      "Sex           418 non-null object\n",
      "Age           418 non-null float64\n",
      "SibSp         418 non-null int64\n",
      "Parch         418 non-null int64\n",
      "Fare          418 non-null float64\n",
      "Embarked      418 non-null object\n",
      "FamilySize    418 non-null int64\n",
      "IsAlone       418 non-null int64\n",
      "Title         418 non-null object\n",
      "FareBin       418 non-null category\n",
      "AgeBin        418 non-null category\n",
      "dtypes: category(2), float64(2), int64(5), object(4)\n",
      "memory usage: 36.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "      <th>FareBin</th>\n",
       "      <th>AgeBin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Leyson, Mr. Robert William Norman</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(7.91, 14.454]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sirayanian, Mr. Orsen</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(-0.001, 7.91]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Swift, Mrs. Frederick Joel (Margaret Welles Ba...</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(14.454, 31.0]</td>\n",
       "      <td>(32.0, 48.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Maisner, Mr. Simon</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(7.91, 14.454]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dick, Mrs. Albert Adrian (Vera Gillespie)</td>\n",
       "      <td>female</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(31.0, 512.329]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Bystrom, Mrs. (Karolina)</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(7.91, 14.454]</td>\n",
       "      <td>(32.0, 48.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Pears, Mrs. Thomas (Edith Wearne)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.6000</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>(31.0, 512.329]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Toomey, Miss. Ellen</td>\n",
       "      <td>female</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Miss</td>\n",
       "      <td>(7.91, 14.454]</td>\n",
       "      <td>(48.0, 64.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(7.91, 14.454]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Barton, Mr. David John</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr</td>\n",
       "      <td>(7.91, 14.454]</td>\n",
       "      <td>(16.0, 32.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass                                               Name  \\\n",
       "234         0       2                  Leyson, Mr. Robert William Norman   \n",
       "60          0       3                              Sirayanian, Mr. Orsen   \n",
       "862         1       1  Swift, Mrs. Frederick Joel (Margaret Welles Ba...   \n",
       "464         0       3                                 Maisner, Mr. Simon   \n",
       "781         1       1          Dick, Mrs. Albert Adrian (Vera Gillespie)   \n",
       "865         1       2                           Bystrom, Mrs. (Karolina)   \n",
       "151         1       1                  Pears, Mrs. Thomas (Edith Wearne)   \n",
       "458         1       2                                Toomey, Miss. Ellen   \n",
       "12          0       3                     Saundercock, Mr. William Henry   \n",
       "112         0       3                             Barton, Mr. David John   \n",
       "\n",
       "        Sex   Age  SibSp  Parch     Fare Embarked  FamilySize  IsAlone Title  \\\n",
       "234    male  24.0      0      0  10.5000        S           1        1    Mr   \n",
       "60     male  22.0      0      0   7.2292        C           1        1    Mr   \n",
       "862  female  48.0      0      0  25.9292        S           1        1   Mrs   \n",
       "464    male  28.0      0      0   8.0500        S           1        1    Mr   \n",
       "781  female  17.0      1      0  57.0000        S           2        0   Mrs   \n",
       "865  female  42.0      0      0  13.0000        S           1        1   Mrs   \n",
       "151  female  22.0      1      0  66.6000        S           2        0   Mrs   \n",
       "458  female  50.0      0      0  10.5000        S           1        1  Miss   \n",
       "12     male  20.0      0      0   8.0500        S           1        1    Mr   \n",
       "112    male  22.0      0      0   8.0500        S           1        1    Mr   \n",
       "\n",
       "             FareBin        AgeBin  \n",
       "234   (7.91, 14.454]  (16.0, 32.0]  \n",
       "60    (-0.001, 7.91]  (16.0, 32.0]  \n",
       "862   (14.454, 31.0]  (32.0, 48.0]  \n",
       "464   (7.91, 14.454]  (16.0, 32.0]  \n",
       "781  (31.0, 512.329]  (16.0, 32.0]  \n",
       "865   (7.91, 14.454]  (32.0, 48.0]  \n",
       "151  (31.0, 512.329]  (16.0, 32.0]  \n",
       "458   (7.91, 14.454]  (48.0, 64.0]  \n",
       "12    (7.91, 14.454]  (16.0, 32.0]  \n",
       "112   (7.91, 14.454]  (16.0, 32.0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###CREATE: Feature Engineering for train and test/validation dataset\n",
    "for dataset in data_cleaner:    \n",
    "    #Discrete variables\n",
    "    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n",
    "\n",
    "    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n",
    "    dataset['Title'] = dataset['Name'].str.extract('.*, (.*?)\\. .*', expand = False)\n",
    "    dataset['Title'] = dataset['Title'].apply(Title_Dictionary.get)\n",
    "\n",
    "\n",
    "    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n",
    "    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "\n",
    "    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "\n",
    "#preview data again\n",
    "train_df.info()\n",
    "test_df.info()\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat = ['Sex', 'Embarked', 'Title', 'FamilySize', 'FareBin', 'AgeBin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 total features after one-hot encoding.\n"
     ]
    }
   ],
   "source": [
    "cat_df_train_encoded = pd.get_dummies(train_df[cat])\n",
    "#print(features)\n",
    "#print(features.shape)\n",
    "                     \n",
    "#Print the number of features after one-hot encoding\n",
    "encoded = list(cat_df_train_encoded.columns)\n",
    "print \"{} total features after one-hot encoding.\".format(len(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 total features after one-hot encoding.\n"
     ]
    }
   ],
   "source": [
    "cat_df_test_encoded = pd.get_dummies(test_df[cat])\n",
    "#print(features)\n",
    "#print(features.shape)\n",
    "                     \n",
    "#Print the number of features after one-hot encoding\n",
    "encoded = list(cat_df_test_encoded.columns)\n",
    "print \"{} total features after one-hot encoding.\".format(len(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_df_train_encoded = pd.DataFrame(cat_df_train_encoded)\n",
    "cat_df_test_encoded = pd.DataFrame(cat_df_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames_train = [train_df, cat_df_train_encoded]\n",
    "train_df = pd.concat(frames_train, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames_test = [test_df, cat_df_train_encoded]\n",
    "test_df = pd.concat(frames_test, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Drop categorical variables\n",
    "drop_column = ['Name','Sex','Embarked','Fare','SibSp','Parch', 'FareBin','Title','AgeBin','FareBin', 'Age']\n",
    "#for dataset in data_cleaner:    \n",
    "#    dataset.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 24 columns):\n",
      "Pclass                     418 non-null float64\n",
      "FamilySize                 418 non-null float64\n",
      "IsAlone                    418 non-null float64\n",
      "FamilySize                 891 non-null int64\n",
      "Sex_female                 891 non-null uint8\n",
      "Sex_male                   891 non-null uint8\n",
      "Embarked_C                 891 non-null uint8\n",
      "Embarked_Q                 891 non-null uint8\n",
      "Embarked_S                 891 non-null uint8\n",
      "Title_Master               891 non-null uint8\n",
      "Title_Miss                 891 non-null uint8\n",
      "Title_Mr                   891 non-null uint8\n",
      "Title_Mrs                  891 non-null uint8\n",
      "Title_Officer              891 non-null uint8\n",
      "Title_Royalty              891 non-null uint8\n",
      "FareBin_(-0.001, 7.91]     891 non-null uint8\n",
      "FareBin_(7.91, 14.454]     891 non-null uint8\n",
      "FareBin_(14.454, 31.0]     891 non-null uint8\n",
      "FareBin_(31.0, 512.329]    891 non-null uint8\n",
      "AgeBin_(-0.08, 16.0]       891 non-null uint8\n",
      "AgeBin_(16.0, 32.0]        891 non-null uint8\n",
      "AgeBin_(32.0, 48.0]        891 non-null uint8\n",
      "AgeBin_(48.0, 64.0]        891 non-null uint8\n",
      "AgeBin_(64.0, 80.0]        891 non-null uint8\n",
      "dtypes: float64(3), int64(1), uint8(20)\n",
      "memory usage: 45.3 KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove Y variable \n",
    "y = train_df[\"Survived\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop([\"Survived\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sss = StratifiedKFold(n_splits = 4, random_state=42)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "num_trees = 100\n",
    "num_folds = 5\n",
    "scoring = 'roc_auc'\n",
    "seed = 7\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\203014767\\.conda\\envs\\DAND\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split = \\\n",
    "    train_test_split(train_df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.857335 (0.035337)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\203014767\\.conda\\envs\\DAND\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.853267 (0.042621)\n",
      "KNN: 0.830510 (0.041837)\n",
      "RF: 0.823467 (0.041675)\n",
      "NB: 0.835888 (0.038630)\n",
      "SVM: 0.830073 (0.046364)\n",
      "ADA: 0.856797 (0.038114)\n",
      "GB: 0.854948 (0.026402)\n"
     ]
    }
   ],
   "source": [
    "# base case\n",
    "# Spot-Check Algorithms\n",
    "LR_clf = LogisticRegression()\n",
    "LDA_clf = LinearDiscriminantAnalysis()\n",
    "KNN_clf = KNeighborsClassifier()\n",
    "RF_clf = RandomForestClassifier(random_state = 7)\n",
    "NB_clf = GaussianNB()\n",
    "SVC_clf = SVC()\n",
    "ADA_clf = AdaBoostClassifier(random_state = 7)\n",
    "GB_clf = GradientBoostingClassifier()\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LR_clf))\n",
    "models.append(('LDA', LDA_clf))\n",
    "models.append(('KNN', KNN_clf))\n",
    "models.append(('RF',RF_clf))\n",
    "models.append(('NB', NB_clf))\n",
    "models.append(('SVM', SVC_clf))\n",
    "models.append(('ADA', ADA_clf))\n",
    "models.append(('GB', GB_clf))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=sss, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.856696 (0.037969)\n",
      "LDA: 0.853267 (0.042621)\n",
      "KNN: 0.808760 (0.032408)\n",
      "RF: 0.823467 (0.041675)\n",
      "NB: 0.835888 (0.038630)\n",
      "SVM: 0.830510 (0.060216)\n",
      "ADA: 0.856797 (0.038114)\n",
      "GB: 0.854881 (0.026383)\n"
     ]
    }
   ],
   "source": [
    "#Scaled\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('LR', Pipeline([('Scaler', scaler),('LR',LR_clf)])))\n",
    "pipelines.append(('LDA', Pipeline([('Scaler', scaler),('LDA',LDA_clf)])))\n",
    "pipelines.append(('KNN', Pipeline([('Scaler', scaler),('KNN',KNN_clf)])))\n",
    "pipelines.append(('RF', Pipeline([('Scaler', scaler),('RF',RF_clf)])))\n",
    "pipelines.append(('NB', Pipeline([('Scaler', scaler),('NB',NB_clf)])))\n",
    "pipelines.append(('SVM', Pipeline([('Scaler', scaler),('SVM', SVC_clf)])))\n",
    "pipelines.append(('ADA', Pipeline([('Scaler', scaler),('ADA', ADA_clf)])))\n",
    "pipelines.append(('GB', Pipeline([('Scaler', scaler),('GB', GB_clf)])))\n",
    "results_sss = []\n",
    "names_sss = []\n",
    "for name, model in pipelines:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=sss, scoring=scoring)\n",
    "    results_sss.append(cv_results)\n",
    "    names_sss.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LR_clf_tuned_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5be7a60815dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpipelines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpipelines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scaler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLR_clf_tuned_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LDA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scaler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LDA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLDA_clf_tuned_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'KNN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scaler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'KNN'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKNN_clf_tuned_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LR_clf_tuned_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "#Tuned\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('LR', Pipeline([('Scaler', scaler),('LR',LR_clf_tuned_scaled)])))\n",
    "pipelines.append(('LDA', Pipeline([('Scaler', scaler),('LDA',LDA_clf_tuned_scaled)])))\n",
    "pipelines.append(('KNN', Pipeline([('Scaler', scaler),('KNN',KNN_clf_tuned_scaled)])))\n",
    "pipelines.append(('RF', Pipeline([('Scaler', scaler),('RF',RF_clf_tuned_scaled)])))\n",
    "pipelines.append(('NB', Pipeline([('Scaler', scaler),('NB',NB_clf)])))\n",
    "pipelines.append(('SVM', Pipeline([('Scaler', scaler),('SVM', SVC_clf_tuned_scaled)])))\n",
    "pipelines.append(('ADA', Pipeline([('Scaler', scaler),('ADA', ADA_clf_tuned_scaled)])))\n",
    "pipelines.append(('GB', Pipeline([('Scaler', scaler),('GB', GB_clf_tuned_scaled)])))\n",
    "pipelines.append(('Voting', Pipeline([('Scaler', scaler),('Voting', voting_ensemble)])))\n",
    "results_scaled_tune = []\n",
    "names_scaled_tune = []\n",
    "for name, model in pipelines:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=sss, scoring=scoring)\n",
    "    results_scaled_tune.append(cv_results)\n",
    "    names_scaled_tune.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n",
    "\n",
    "# Compare Algorithms Scaled\n",
    "fig_sss = plt.figure()\n",
    "fig_sss.suptitle('Scaled Algorithm Comparison')\n",
    "ax = fig_sss.add_subplot(111)\n",
    "plt.boxplot(results_sss)\n",
    "ax.set_xticklabels(names_sss)\n",
    "plt.show()\n",
    "\n",
    "# Compare Algorithms\n",
    "fig_tune_scaled = plt.figure()\n",
    "fig_tune_scaled.suptitle('Tuning Algorithm Comparison')\n",
    "ax = fig_tune_scaled.add_subplot(111)\n",
    "plt.boxplot(results_scaled_tune, showmeans = True, meanline=False)\n",
    "ax.set_xticklabels(names_scaled_tune)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as 'capital-gain' or 'capital-loss' above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale data between 0 and 1\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled =scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune scaled KNN\n",
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',KNN_clf) \n",
    "         ])\n",
    "param_grid = dict(clf__n_neighbors=range(1,12))\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoring, cv=sss)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "KNN_clf_tuned_scaled = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune scaled SVM\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "prob =[True, False]\n",
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',SVC_clf) \n",
    "         ])\n",
    "param_grid = dict(clf__C=c_values, clf__kernel=kernel_values, clf__probability = prob)\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoring, cv=sss)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "SVC_clf_tuned_scaled = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune LR\n",
    "c_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "penalty_values = ['l1', 'l2']\n",
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',LR_clf) \n",
    "         ])\n",
    "param_grid = dict(clf__C=c_values, clf__penalty=penalty_values)\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoring, cv=sss)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "LR_clf_tuned_scaled = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune LDA\n",
    "solver_values = ['svd', 'lsqr']\n",
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',LDA_clf) \n",
    "         ])\n",
    "param_grid = dict(clf__solver=solver_values)\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoring, cv=sss)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "LDA_clf_tuned_scaled = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LDA_clf_tuned_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "#Tune RF\n",
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',RF_clf) \n",
    "         ])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__max_depth' : randint(low=1, high=5),\n",
    "    'clf__max_features':uniform(),\n",
    "    'clf__min_samples_split': randint(low=2, high=5),\n",
    "    'clf__n_estimators' : randint(low=100, high=200) }\n",
    "\n",
    "rsearch = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=25, cv = sss, \n",
    "                             random_state=7, scoring = scoring)\n",
    "rsearch_result = rsearch.fit(X_train, y_train)\n",
    "#print(rsearch.best_score_)\n",
    "#print(rsearch.best_estimator_)\n",
    "#print(rsearch.best_params_)\n",
    "\n",
    "print(\"Best: %f using %s\" % (rsearch_result.best_score_, rsearch_result.best_params_))\n",
    "RF_clf_tuned_scaled = rsearch_result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: 0.860946 using {'clf__max_features': 0.90912837488673126, 'clf__max_depth': 3, 'clf__n_estimators': 187, 'clf__min_samples_split': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune ADA - First tune number of Trees\n",
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',ADA_clf) \n",
    "         ])\n",
    "param_grid = {#'learning_rate' : uniform(),            \n",
    "'clf__n_estimators' : range(1, 100, 5) }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoring, cv=sss)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "ADA_clf_tuned_scaled = grid.best_estimator_\n",
    "#means = grid_result.cv_results_['mean_test_score']\n",
    "#stds = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "#ADA_clf_tuned = grid_search.best_estimator_\n",
    "#print(rsearch.best_score_)\n",
    "#print(rsearch.best_estimator_)\n",
    "#print(rsearch.best_params_)\n",
    "\n",
    "#print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#GB_clf_tuned = grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GB_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune GB - First tune number of Trees\n",
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',GB_clf) \n",
    "         ])\n",
    "param_grid = {#'learning_rate' : uniform(), \n",
    "'clf__max_depth' : [2, 4, 6, 8, 10],\n",
    "#'subsample': uniform(),            \n",
    "'clf__n_estimators' : range(100, 1000, 100) }\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring=scoring, cv=sss)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "GB_clf_tuned_scaled = grid.best_estimator_\n",
    "#means = grid_result.cv_results_['mean_test_score']\n",
    "#stds = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GB_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline =Pipeline([\n",
    "        ('Scaler', scaler),\n",
    "        ('clf',GB_clf) \n",
    "         ])\n",
    "\n",
    "param_grid = {'clf__learning_rate' : uniform(0.02,0.1),\n",
    "'clf__objective': ['binary:logistic'],\n",
    "'clf__gamma': [0],\n",
    "'clf__min_child_weight': uniform(),              \n",
    "'clf__max_depth': randint(low=6, high=12),\n",
    "'clf__subsample': [1.0],#row sampling              \n",
    "'clf__colsample_bytree':[0.3, 0.4, 0.5],\n",
    "'clf__n_estimators' : [100] }\n",
    "\n",
    "\n",
    "rsearch = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=50, cv = sss, \n",
    "                             random_state=7, scoring = scoring)\n",
    "\n",
    "rsearch_result = rsearch.fit(X_train, y_train)\n",
    "print(\"Best: %f using %s\" % (rsearch_result.best_score_, rsearch_result.best_params_))\n",
    "GB_clf_tuned_scaled = rsearch_result.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: 0.876123 using {'clf__gamma': 2.7159522025956386, 'clf__n_estimators': 103, 'clf__max_depth': 9, 'clf__learning_rate': 0.27721958269429525, 'clf__objective': 'binary:logistic', 'clf__subsample': 0.69359919451504393}\n",
    "\n",
    "Best: 0.868762 using {'clf__gamma': 0, 'clf__n_estimators': 100, 'clf__max_depth': 10, 'clf__learning_rate': 0.023552880483036338, 'clf__min_child_weight': 0.38679069137477007, 'clf__colsample_bytree': 0.4, 'clf__objective': 'binary:logistic', 'clf__subsample': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Voting\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = RF_clf_tuned_scaled\n",
    "estimators.append(('RF', model1))\n",
    "#model2 = LR_clf_tuned_scaled\n",
    "#estimators.append(('LR', model2))\n",
    "model3 = GB_clf_tuned_scaled\n",
    "estimators.append(('GB', model3))\n",
    "#model4 = ADA_clf_tuned_scaled\n",
    "#estimators.append(('ADA', model3))\n",
    "# create the ensemble model\n",
    "voting_ensemble = VotingClassifier(estimators, voting = 'soft' )\n",
    "results = cross_val_score(voting_ensemble, X_train_scaled, y_train, cv = sss, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves (LR Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(LR_clf_tuned_scaled, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "\n",
    "title = \"Learning Curves (LDA Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(LDA_clf_tuned_scaled, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "\n",
    "title = \"Learning Curves (KNN Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(KNN_clf_tuned_scaled, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "\n",
    "title = \"Learning Curves (NB)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(NB_clf, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "title = \"Learning Curves (SVC Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(SVC_clf_tuned_scaled, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "title = \"Learning Curves (ADA Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(ADA_clf, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves (GB UnTuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(GB_clf, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "\n",
    "title = \"Learning Curves (RF Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(RF_clf_tuned_scaled, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "title = \"Learning Curves (GB Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(GB_clf_tuned_scaled, title, X_train_scaled, y_train, cv=sss, n_jobs=1)\n",
    "\n",
    "title = \"Learning Curves (Voting Tuned)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "plot_learning_curve(voting_ensemble, title, X_train_scaled, y_train, cv=sss, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_clf_tuned_scaled.fit(X_train_scaled, y_train)\n",
    "RF_test_pred = RF_clf_tuned_scaled.predict(X_test_scaled)\n",
    "RF_prec_test = precision_score(RF_test_pred, y_test)\n",
    "RF_recall_test = recall_score(RF_test_pred, y_test)\n",
    "RF_f1_test = f1_score(RF_test_pred, y_test)\n",
    "RF_ROC_AUC_test = roc_auc_score(RF_test_pred, y_test)\n",
    "\n",
    "print(\"Precision Test: {}\".format(RF_prec_test))\n",
    "print (\"Recall Test: {}\".format(RF_recall_test))\n",
    "print (\"F1 Test: {}\".format(RF_f1_test))\n",
    "print (\"ROC AUC Test: {}\".format(RF_ROC_AUC_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR_clf_tuned_scaled.fit(X_train_scaled, y_train)\n",
    "SVC_test_pred = LR_clf_tuned_scaled.predict(X_test_scaled)\n",
    "SVC_prec_test = precision_score(SVC_test_pred, y_test)\n",
    "SVC_recall_test = recall_score(SVC_test_pred, y_test)\n",
    "SVC_f1_test = f1_score(SVC_test_pred, y_test)\n",
    "SVC_ROC_AUC_test = roc_auc_score(SVC_test_pred, y_test)\n",
    "\n",
    "print(\"Precision Test: {}\".format(SVC_prec_test))\n",
    "print (\"Recall Test: {}\".format(SVC_recall_test))\n",
    "print (\"F1 Test: {}\".format(SVC_f1_test))\n",
    "print (\"ROC AUC Test: {}\".format(SVC_ROC_AUC_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GB_clf_tuned_scaled.fit(X_train_scaled, y_train)\n",
    "GB_test_pred = GB_clf_tuned_scaled.predict(X_test_scaled)\n",
    "GB_prec_test = precision_score(GB_test_pred, y_test)\n",
    "GB_recall_test = recall_score(GB_test_pred, y_test)\n",
    "GB_f1_test = f1_score(GB_test_pred, y_test)\n",
    "GB_ROC_AUC_test = roc_auc_score(GB_test_pred, y_test)\n",
    "\n",
    "print(\"GBC Precision Test: {}\".format(GB_prec_test))\n",
    "print (\"GBC Recall Test: {}\".format(GB_recall_test))\n",
    "print (\"GBC F1 Test: {}\".format(GB_f1_test))\n",
    "print (\"GBC ROC AUC Test: {}\".format(GB_ROC_AUC_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ADA_clf_tuned_scaled.fit(X_train_scaled, y_train)\n",
    "ADA_test_pred = ADA_clf_tuned_scaled.predict(X_test_scaled)\n",
    "ADA_prec_test = precision_score(ADA_test_pred, y_test)\n",
    "ADA_recall_test = recall_score(ADA_test_pred, y_test)\n",
    "ADA_f1_test = f1_score(ADA_test_pred, y_test)\n",
    "ADA_ROC_AUC_test = roc_auc_score(ADA_test_pred, y_test)\n",
    "\n",
    "print(\"Precision Test: {}\".format(ADA_prec_test))\n",
    "print (\"Recall Test: {}\".format(ADA_recall_test))\n",
    "print (\"F1 Test: {}\".format(ADA_f1_test))\n",
    "print (\"ROC AUC Test: {}\".format(ADA_ROC_AUC_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voting_ensemble\n",
    "\n",
    "voting_ensemble.fit(X_train_scaled, y_train)\n",
    "voting_test_pred = voting_ensemble.predict(X_test_scaled)\n",
    "voting_prec_test = precision_score(voting_test_pred, y_test)\n",
    "voting_recall_test = recall_score(voting_test_pred, y_test)\n",
    "voting_f1_test = f1_score(voting_test_pred, y_test)\n",
    "voting_ROC_AUC_test = roc_auc_score(voting_test_pred, y_test)\n",
    "\n",
    "print(\"Precision Test: {}\".format(voting_prec_test))\n",
    "print (\"Recall Test: {}\".format(voting_recall_test))\n",
    "print (\"F1 Test: {}\".format(voting_f1_test))\n",
    "print (\"ROC AUC Test: {}\".format(voting_ROC_AUC_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and Transform Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_scaled  = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = voting_ensemble.predict(test_scaled)\n",
    "print(y_pred)\n",
    "\n",
    "# Generate Submission File \n",
    "submission = pd.DataFrame({ 'PassengerId': Id,\n",
    "                            'Survived': y_pred })\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
